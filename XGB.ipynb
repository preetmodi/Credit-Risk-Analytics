{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOSWQbGmG0VLcJ8cwKLzD28",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/preetmodi/Credit-Risk-Analytics/blob/main/XGB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo echo -ne '\\n' | sudo add-apt-repository ppa:alessandro-strada/ppa >/dev/null 2>&1 # note: >/dev/null 2>&1 is used to supress printing\n",
        "!sudo apt update >/dev/null 2>&1\n",
        "!sudo apt install google-drive-ocamlfuse >/dev/null 2>&1\n",
        "!google-drive-ocamlfuse\n",
        "!sudo apt-get install w3m >/dev/null 2>&1 # to act as web browser \n",
        "!xdg-settings set default-web-browser w3m.desktop >/dev/null 2>&1 # to set default browser \n",
        "%cd /content\n",
        "!mkdir gdrive\n",
        "%cd gdrive\n",
        "!mkdir \"My Drive\"\n",
        "!google-drive-ocamlfuse \"/content/gdrive/My Drive\""
      ],
      "metadata": {
        "id": "FEzfP2Z5KtVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/CBravoR/scorecardpy\n",
        "\n",
        "%cd /content/gdrive/My Drive/Data/"
      ],
      "metadata": {
        "id": "RkliXOevKvln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Package loading\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scorecardpy as sc\n",
        "from string import ascii_letters\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "%matplotlib inline\n",
        "seed = 251256517"
      ],
      "metadata": {
        "id": "UVYudb9PKxUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the files as Pandas datasets\n",
        "# train_WoE = pd.read_csv('train_woe.csv')\n",
        "# test_WoE = pd.read_csv('test_woe.csv')\n",
        "\n",
        "train_noWoE = pd.read_csv('train.csv')\n",
        "test_noWoE = pd.read_csv('test.csv')\n",
        "\n",
        "# Give breaks for WoE\n",
        "with open('breaks_adj.json', 'r') as f:\n",
        "    breaks_adj = json.load(f)\n",
        "# breaks_adj = {'Address': [1.0,2.0,8.0,17.0],\n",
        "#               'Age': [30.0,45.0,50.0],\n",
        "#               'Creddebt': [1.0, 6.0],\n",
        "#               'Employ': [4.0,14.0,22.0],\n",
        "#               'Income': [30.0,40.0,80.0,140.0],\n",
        "#               'Leverage': [8.0,16.0,22.0],\n",
        "#               'MonthlyLoad': [0.1,0.2,0.30000000000000004,0.7000000000000001],\n",
        "#               'OthDebt': [1.0,2.0,3.0],\n",
        "#               'OthDebtRatio': [0.01,0.05,0.07,0.09,0.14]\n",
        "#               }\n",
        "\n",
        "# Apply breaks.\n",
        "bins_adj = sc.woebin(train_noWoE, y=\"Default\",\n",
        "                     breaks_list=breaks_adj)\n",
        "  "
      ],
      "metadata": {
        "id": "5axv9sPjLJTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1oJ8K6AKmSJ"
      },
      "outputs": [],
      "source": [
        "from xgboost import XGBClassifier \n",
        "#Define the classifier.\n",
        "XGB_lendingclub = XGBClassifier(max_depth=2,                 # Depth of each tree\n",
        "                            learning_rate=0.1,            # How much to shrink error in each subsequent training. Trade-off with no. estimators.\n",
        "                            n_estimators=50,             # How many trees to use, the more the better, but decrease learning rate if many used.\n",
        "                            verbosity=1,                  # If to show more errors or not.\n",
        "                            objective='binary:logistic',  # Type of target variable.\n",
        "                            booster='gbtree',             # What to boost. Trees in this case.\n",
        "                            n_jobs=2,                    # Parallel jobs to run. Set your processor number.\n",
        "                            gamma=0.001,                  # Minimum loss reduction required to make a further partition on a leaf node of the tree. (Controls growth!)\n",
        "                            subsample=0.632,              # Subsample ratio. Can set lower\n",
        "                            colsample_bytree=1,           # Subsample ratio of columns when constructing each tree.\n",
        "                            colsample_bylevel=1,          # Subsample ratio of columns when constructing each level. 0.33 is similar to random forest.\n",
        "                            colsample_bynode=1,           # Subsample ratio of columns when constructing each split.\n",
        "                            reg_alpha=1,                  # Regularizer for first fit. alpha = 1, lambda = 0 is LASSO.\n",
        "                            reg_lambda=0,                 # Regularizer for first fit.\n",
        "                            scale_pos_weight=1,           # Balancing of positive and negative weights. G / B\n",
        "                            base_score=0.5,               # Global bias. Set to average of the target rate.\n",
        "                            random_state=seed,        # Seed\n",
        "                            tree_method='hist',           # How to train the trees?\n",
        "                            #gpu_id=0                     # With which GPU? \n",
        "                            )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Change these"
      ],
      "metadata": {
        "id": "lB4AW9GkK_xv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = dict({'n_estimators': [50, 100, 150],\n",
        "                   'max_depth': [2, 3, 4],\n",
        "                 'learning_rate' : [0.01, 0.05, 0.1, 0.15]\n",
        "                  })"
      ],
      "metadata": {
        "id": "dDdsmw2cK_Ol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Always a good idea to tune on a reduce sample of the train set, as we will call many functions.\n",
        "val_train = train_noWoE.sample(frac = 0.5,               # The fraction to extract\n",
        "                                       random_state = seed,    # The seed.\n",
        "                                       )"
      ],
      "metadata": {
        "id": "e3E0UGJfLDHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run Grid Search\n"
      ],
      "metadata": {
        "id": "sikMrorCLPO-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define grid search object.\n",
        "GridXGB = GridSearchCV(XGB_lendingclub,        # Original XGB. \n",
        "                       param_grid,          # Parameter grid\n",
        "                       cv = 3,              # Number of cross-validation folds.  \n",
        "                       scoring = 'roc_auc', # How to rank outputs.\n",
        "                       n_jobs = 2,          # Parallel jobs. -1 is \"all you have\"\n",
        "                       refit = False,       # If refit at the end with the best. We'll do it manually.\n",
        "                       verbose = 1          # If to show what it is doing.\n",
        "                      )"
      ],
      "metadata": {
        "id": "SqzAhCMgLOia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Follow the same approach for all categorical Variables"
      ],
      "metadata": {
        "id": "arCFllWhLV_z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_features = [\"Education\"]\n",
        "categorical_transformer = OneHotEncoder(handle_unknown=\"ignore\")\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[(\"cat\", categorical_transformer, categorical_features)],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "# Now we define a Pipeline to process everything\n",
        "clf = Pipeline(\n",
        "    steps=[(\"preprocessor\", preprocessor), (\"classifier\", GridXGB)]\n",
        ")\n",
        "\n",
        "# Train the XGB.\n",
        "clf.fit(val_train.drop(columns='Default'), # X \n",
        "        val_train['Default']    # y\n",
        "        )"
      ],
      "metadata": {
        "id": "QnUBZeO_LC8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show best params\n",
        "print(f'The best AUC is {GridXGB.best_score_:.3f}')\n",
        "GridXGB.best_params_"
      ],
      "metadata": {
        "id": "aq-ZUtLULdgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train model on entire training dataset"
      ],
      "metadata": {
        "id": "wUTKn4R6LeZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create XGB with best parameters.\n",
        "XGB_lendingclub = XGBClassifier(max_depth=GridXGB.best_params_.get('max_depth'), # Depth of each tree\n",
        "                            learning_rate=GridXGB.best_params_.get('learning_rate'), # How much to shrink error in each subsequent training. Trade-off with no. estimators.\n",
        "                            n_estimators=GridXGB.best_params_.get('n_estimators'), # How many trees to use, the more the better, but decrease learning rate if many used.\n",
        "                            verbosity=1,                  # If to show more errors or not.\n",
        "                            objective='binary:logistic',  # Type of target variable.\n",
        "                            booster='gbtree',             # What to boost. Trees in this case.\n",
        "                            n_jobs=2,                     # Parallel jobs to run. Set your processor number.\n",
        "                            gamma=0.001,                  # Minimum loss reduction required to make a further partition on a leaf node of the tree. (Controls growth!)\n",
        "                            subsample=0.632,              # Subsample ratio. Can set lower\n",
        "                            colsample_bytree=1,           # Subsample ratio of columns when constructing each tree.\n",
        "                            colsample_bylevel=1,          # Subsample ratio of columns when constructing each level. 0.33 is similar to random forest.\n",
        "                            colsample_bynode=1,           # Subsample ratio of columns when constructing each split.\n",
        "                            reg_alpha=1,                  # Regularizer for first fit. alpha = 1, lambda = 0 is LASSO.\n",
        "                            reg_lambda=0,                 # Regularizer for first fit.\n",
        "                            scale_pos_weight=1,           # Balancing of positive and negative weights.\n",
        "                            base_score=0.5,               # Global bias. Set to average of the target rate.\n",
        "                            random_state=20201107,        # Seed\n",
        "                            tree_method='gpu_hist',       # How to train the trees?\n",
        "                            gpu_id=0                      # With which GPU?\n",
        "                            )"
      ],
      "metadata": {
        "id": "8qIX-Rh8LgJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train over all training data.\n",
        "# Create dummy variables for education\n",
        "categorical_features = [\"Education\"]\n",
        "categorical_transformer = OneHotEncoder(handle_unknown=\"ignore\")\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[(\"cat\", categorical_transformer, categorical_features)],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "# Now we define a Pipeline to process everything\n",
        "clf = Pipeline(\n",
        "    steps=[(\"preprocessor\", preprocessor), (\"classifier\", XGB_lendingclub)]\n",
        ")\n",
        "\n",
        "# Train the XGB.\n",
        "clf.fit(train_noWoE.drop(columns='Default'), # X \n",
        "                  train_noWoE['Default']    # y\n",
        "                  )"
      ],
      "metadata": {
        "id": "CpNyBj0LLk7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Variable Importance"
      ],
      "metadata": {
        "id": "Vt_-u16sLojs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot variable importance\n",
        "importances = XGB_lendingclub.feature_importances_\n",
        "indices = np.argsort(importances)[::-1] \n",
        "\n",
        "f, ax = plt.subplots(figsize=(3, 8))\n",
        "plt.title(\"Variable Importance - XGBoosting\")\n",
        "sns.set_color_codes(\"pastel\")\n",
        "sns.barplot(y=[clf[:-1].get_feature_names_out()[i] for i in indices], x=importances[indices], \n",
        "            label=\"Total\", color=\"b\")\n",
        "ax.set(ylabel=\"Variable\",\n",
        "       xlabel=\"Variable Importance (Entropy)\")\n",
        "sns.despine(left=True, bottom=True)"
      ],
      "metadata": {
        "id": "5a523u7HLoCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Confusion Matrix"
      ],
      "metadata": {
        "id": "dNrrX5NrLtYW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate probability\n",
        "XGBClassTest = clf.predict(test_noWoE.drop(columns=\"Default\"))\n",
        "xg_probs_test = clf.predict_proba(test_noWoE.drop(columns=\"Default\"))\n",
        "xg_probs_test = xg_probs_test[:, 1]\n",
        "\n",
        "# Calculate confusion matrix\n",
        "confusion_matrix_xgb = confusion_matrix(y_true = test_noWoE['Default'], \n",
        "                    y_pred = XGBClassTest)\n",
        "\n",
        "# Turn matrix to percentages\n",
        "confusion_matrix_xgb = confusion_matrix_xgb.astype('float') / confusion_matrix_xgb.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "# Turn to dataframe\n",
        "df_cm = pd.DataFrame(\n",
        "        confusion_matrix_xgb, index=['good', 'bad'], columns=['good', 'bad'], \n",
        ")\n",
        "\n",
        "# Parameters of the image\n",
        "figsize = (10,7)\n",
        "fontsize=14\n",
        "\n",
        "# Create image\n",
        "fig = plt.figure(figsize=figsize)\n",
        "heatmap = sns.heatmap(df_cm, annot=True, fmt='.2f')\n",
        "\n",
        "# Make it nicer\n",
        "heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, \n",
        "                             ha='right', fontsize=fontsize)\n",
        "heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45,\n",
        "                             ha='right', fontsize=fontsize)\n",
        "\n",
        "# Add labels\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "\n",
        "# Plot!\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JHEccf49Luw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ROC AUC"
      ],
      "metadata": {
        "id": "H-omTIEXLxTo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the ROC curve points\n",
        "fpr, tpr, thresholds = roc_curve(test_noWoE['Default'], \n",
        "                                 xg_probs_test)\n",
        "\n",
        "# Save the AUC in a variable to display it. Round it first\n",
        "auc = np.round(roc_auc_score(y_true = test_noWoE['Default'], \n",
        "                             y_score = xg_probs_test),\n",
        "               decimals = 3)\n",
        "\n",
        "# Create and show the plot\n",
        "plt.plot(fpr,tpr,label=\"AUC - XGBoosting = \" + str(auc))\n",
        "plt.legend(loc=4)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "y4A3rajZLyNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bootstapping to calculate error rates"
      ],
      "metadata": {
        "id": "3om09if3L1zW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply bootstrapping\n",
        "auc_boot = BootstrapPredErr(pred_prob=xg_probs_test,  \n",
        "                            y_true=test_noWoE['Default'])\n",
        "upper=np.quantile(auc-auc_boot, 0.975, axis=0)\n",
        "lower=np.quantile(auc-auc_boot, 0.025, axis=0)\n",
        "\n",
        "# Compute confidence interval\n",
        "boot_ci = [auc - upper, \n",
        "           auc - lower]\n",
        "print(f\"AUC Confidence Interval: [{boot_ci[0]:.3f}, {boot_ci[1]:.3f}]\")"
      ],
      "metadata": {
        "id": "G3M0pehtL4k-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Shap Values"
      ],
      "metadata": {
        "id": "Ei8r690FL7RF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install shap"
      ],
      "metadata": {
        "id": "5V3Gf06GL9md"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "shap.initjs() # Import Java engine."
      ],
      "metadata": {
        "id": "Q3Sa_vrxL_Zf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create transformed data from previous preprocessor\n",
        "data = pd.DataFrame(preprocessor.fit_transform(test_noWoE.drop(columns=\"Default\")))\n",
        "data.columns = preprocessor.get_feature_names_out()\n",
        "\n",
        "\n",
        "# Trains the game-theoretic model. Really complex so requires sampling.\n",
        "explainer = shap.TreeExplainer(XGB_lendingclub,                 # The model    \n",
        "                              data = shap.sample(data, 100)  # Create a sample of 100 cases\n",
        "                              )\n",
        "\n",
        "# Applies model ot the full dataset.\n",
        "shap_values = explainer.shap_values(data, check_additivity=False)"
      ],
      "metadata": {
        "id": "_1qDK9HIL_QR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shap.summary_plot(shap_values,                       # The Shapley values.\n",
        "                  data, # The training sample\n",
        "                  show=False)                        # Whether to print the model or not\n",
        "\n",
        "# Let's save this as a PDF for later use.\n",
        "plt.savefig('ShapSummaryPlot.pdf', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UsIqnELBMEVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shap.dependence_plot(\"remainder__Employ\",                          # The variable to study\n",
        "                     shap_values,                       # The Shapley values.\n",
        "                     data, # The training sample\n",
        "                     show=False)                        # Whether to print the model or not\n",
        "\n",
        "plt.savefig('ShapEmploy.pdf', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-8Dp0H5TMGeC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compare Models"
      ],
      "metadata": {
        "id": "n8HOaZZEMPV3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict probabilities of scorecard.\n",
        "logreg_probs_test = logreg.predict_proba(test_WoE.iloc[:, 1:])"
      ],
      "metadata": {
        "id": "R1lVJPd5MRcF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set models and probabilities. This structure is called a dictionary.\n",
        "models = [\n",
        "{\n",
        "    'label': 'Logistic Regression',\n",
        "    'probs': logreg_probs_test[:,1]\n",
        "},\n",
        "{\n",
        "    'label': 'Gradient Boosting',\n",
        "    'probs': xg_probs_test\n",
        "},\n",
        "\n",
        "\n",
        "# Loop that creates the plot. I will pass each ROC curve one by one.\n",
        "for m in models:\n",
        "  auc = roc_auc_score(y_true = test_noWoE['Default'], \n",
        "                             y_score = m['probs'])\n",
        "  fpr, tpr, thresholds = roc_curve(test_WoE['Default'], \n",
        "                                           m['probs'])\n",
        "  plt.plot(fpr, tpr, label=f'{m[\"label\"]} ROC (area = {auc:.3f})')\n",
        "                 \n",
        "\n",
        "    \n",
        "# Settings\n",
        "plt.plot([0, 1], [0, 1],'r--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('1-Specificity(False Positive Rate)')\n",
        "plt.ylabel('Sensitivity(True Positive Rate)')\n",
        "plt.title('Receiver Operating Characteristic')\n",
        "plt.legend(loc=\"lower right\")\n",
        "    \n",
        "# Plot!    \n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7EX7YsQ0MTCE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}